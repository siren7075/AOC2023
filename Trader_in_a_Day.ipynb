{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siren7075/AOC2023/blob/main/Trader_in_a_Day.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oWbj4HgqHBg"
      },
      "source": [
        "# Part 1. Install and LoadPackages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0RbVjUMoheu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#12min\n",
        "!pip install git+https://github.com/benstaf/FinRL.git\n",
        "!pip install selenium webdriver-manager alpaca-py datasets\n",
        "!git clone https://github.com/benstaf/FinRL_DeepSeek.git\n",
        "%cd FinRL_DeepSeek\n",
        "!bash installation_script.sh\n",
        "!pip install transformers datasets huggingface_hub stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "import shutil\n",
        "\n",
        "# Download the entire repo\n",
        "local_dir = snapshot_download(repo_id=\"benstaf/Trading_agents\")\n",
        "\n",
        "# Optional: move all files to root /content\n",
        "shutil.copytree(local_dir, \"/content\", dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "mw_UVy9jXXOv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqfBOKz-qJYF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "\n",
        "#from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR\n",
        "from env_stocktrading import StockTradingEnv\n",
        "\n",
        "\n",
        "# Import PPO-DeepSeek environments\n",
        "from env_stocktrading_llm import StockTradingEnv as StockTradingEnv_llm\n",
        "from env_stocktrading_llm_1 import StockTradingEnv as StockTradingEnv_llm_1\n",
        "from env_stocktrading_llm_01 import StockTradingEnv as StockTradingEnv_llm_01\n",
        "\n",
        "# Import CPPO-DeepSeek risk environments\n",
        "from env_stocktrading_llm_risk import StockTradingEnv as StockTradingEnv_llm_risk\n",
        "from env_stocktrading_llm_risk_1 import StockTradingEnv as StockTradingEnv_llm_risk_1\n",
        "from env_stocktrading_llm_risk_01 import StockTradingEnv as StockTradingEnv_llm_risk_01\n",
        "\n",
        "#from env_stocktrading_llm import StockTradingEnv as StockTradingEnv_llm\n",
        "\n",
        "#from env_stocktrading_llm_risk import StockTradingEnv as StockTradingEnv_llm_risk\n",
        "\n",
        "\n",
        "#from finrl.meta.env_stock_trading.env_stocktrading_llm import StockTradingEnv\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Prepare for Models"
      ],
      "metadata": {
        "id": "u4PVw-zvOag-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from Huggging Face :\n",
        "dataset = load_dataset(\"benstaf/nasdaq_2013_2023\", data_files='trade_data_deepseek_sentiment_2019_2023.csv')\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "trade = pd.DataFrame(dataset['train'])\n",
        "\n",
        "#trade= pd.read_csv('/content/machine_learning/trade_data_qwen_sentiment.csv')\n",
        "\n",
        "trade = trade.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "# Create a new index based on unique dates\n",
        "unique_dates = trade['date'].unique()\n",
        "date_to_idx = {date: idx for idx, date in enumerate(unique_dates)}\n",
        "\n",
        "# Create new index based on the date mapping\n",
        "trade['new_idx'] = trade['date'].map(date_to_idx)\n",
        "\n",
        "# Set this as the index\n",
        "trade = trade.set_index('new_idx')\n",
        "\n",
        "\n",
        "#missing values with 0\n",
        "trade['llm_sentiment'].fillna(0, inplace=True)\n",
        "trade_llm=trade\n",
        "\n",
        "\n",
        "#trade = pd.read_csv('/content/machine_learning/trade_data_qwen_risk.csv')\n",
        "\n",
        "# from Huggging Face :\n",
        "dataset = load_dataset(\"benstaf/nasdaq_2013_2023\", data_files='trade_data_deepseek_risk_2019_2023.csv')\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "trade = pd.DataFrame(dataset['train'])\n",
        "\n",
        "trade = trade.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "# Create a new index based on unique dates\n",
        "unique_dates = trade['date'].unique()\n",
        "date_to_idx = {date: idx for idx, date in enumerate(unique_dates)}\n",
        "\n",
        "# Create new index based on the date mapping\n",
        "trade['new_idx'] = trade['date'].map(date_to_idx)\n",
        "\n",
        "# Set this as the index\n",
        "trade = trade.set_index('new_idx')\n",
        "\n",
        "\n",
        "#missing values with 0\n",
        "trade['llm_sentiment'].fillna(0, inplace=True)\n",
        "#missing values with 3\n",
        "trade['llm_risk'].fillna(3, inplace=True)\n",
        "trade_llm_risk=trade\n",
        "\n",
        "#trade = pd.read_csv('/content/machine_learning/trade_data_qwen_risk.csv')\n",
        "\n",
        "# from Huggging Face :\n",
        "dataset = load_dataset(\"benstaf/nasdaq_2013_2023\", data_files='trade_data_2019_2023.csv')\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "trade = pd.DataFrame(dataset['train'])\n",
        "\n",
        "trade = trade.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "# Create a new index based on unique dates\n",
        "unique_dates = trade['date'].unique()\n",
        "date_to_idx = {date: idx for idx, date in enumerate(unique_dates)}\n",
        "\n",
        "# Create new index based on the date mapping\n",
        "trade['new_idx'] = trade['date'].map(date_to_idx)\n",
        "\n",
        "# Set this as the index\n",
        "trade = trade.set_index('new_idx')\n",
        "\n"
      ],
      "metadata": {
        "id": "u1BssNtcRmvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_dimension = len(trade.tic.unique())\n",
        "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension #+ stock_dimension # +LLM sentiment\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
        "\n",
        "stock_dimension_llm = len(trade_llm.tic.unique())\n",
        "state_space_llm = 1 + 2 * stock_dimension_llm + (1+len(INDICATORS)) * stock_dimension_llm #+ stock_dimension # +LLM sentiment\n",
        "print(f\"Stock Dimension: {stock_dimension_llm}, State Space: {state_space_llm}\")\n",
        "\n",
        "stock_dimension = len(trade.tic.unique())\n",
        "state_space_llm_risk = 1 + 2 * stock_dimension + (2+len(INDICATORS)) * stock_dimension #+ stock_dimension # +LLM sentiment + LLM risk\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space_llm_risk}\")\n",
        "\n",
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "buy_cost_list_llm = sell_cost_list_llm = [0.001] * stock_dimension_llm\n",
        "num_stock_shares_llm = [0] * stock_dimension_llm\n",
        "\n",
        "env_kwargs_llm = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares_llm,\n",
        "    \"buy_cost_pct\": buy_cost_list_llm,\n",
        "    \"sell_cost_pct\": sell_cost_list_llm,\n",
        "    \"state_space\": state_space_llm,\n",
        "    \"stock_dim\": stock_dimension_llm,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension_llm,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs_llm_risk = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space_llm_risk,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs)\n",
        "# env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
        "\n",
        "e_trade_llm_gym = StockTradingEnv_llm(df = trade_llm, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs_llm)\n",
        "# env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
        "\n",
        "\n",
        "# Environment for PPO-DeepSeek 10%\n",
        "e_trade_llm_gym = StockTradingEnv_llm(df=trade_llm, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm)\n",
        "\n",
        "# Environment for PPO-DeepSeek 1%\n",
        "e_trade_llm_gym_1 = StockTradingEnv_llm_1(df=trade_llm, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm)\n",
        "\n",
        "# Environment for PPO-DeepSeek 0.1%\n",
        "e_trade_llm_gym_01 = StockTradingEnv_llm_01(df=trade_llm, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm)\n",
        "\n",
        "e_trade_llm_risk_gym = StockTradingEnv_llm_risk(df = trade_llm_risk, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs_llm_risk)\n",
        "\n",
        "# Environment for CPPO-DeepSeek 10% risk\n",
        "e_trade_llm_risk_gym = StockTradingEnv_llm_risk(df=trade_llm_risk, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm_risk)\n",
        "\n",
        "# Environment for CPPO-DeepSeek 1% risk\n",
        "e_trade_llm_risk_gym_1 = StockTradingEnv_llm_risk_1(df=trade_llm_risk, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm_risk)\n",
        "\n",
        "# Environment for CPPO-DeepSeek 0.1% risk\n",
        "e_trade_llm_risk_gym_01 = StockTradingEnv_llm_risk_01(df=trade_llm_risk, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm_risk)\n",
        "\n",
        "observation_space=e_trade_gym.observation_space\n",
        "action_space=e_trade_gym.action_space\n",
        "\n",
        "observation_space_llm=e_trade_llm_gym.observation_space\n",
        "action_space_llm=e_trade_llm_gym.action_space\n",
        "\n",
        "observation_space_llm_risk=e_trade_llm_risk_gym.observation_space\n",
        "action_space_llm_risk=e_trade_llm_risk_gym.action_space\n",
        "\n",
        "# Observation and action spaces for PPO-DeepSeek 10%\n",
        "observation_space_llm = e_trade_llm_gym.observation_space\n",
        "action_space_llm = e_trade_llm_gym.action_space\n",
        "\n",
        "# Observation and action spaces for PPO-DeepSeek 1%\n",
        "observation_space_llm_1 = e_trade_llm_gym_1.observation_space\n",
        "action_space_llm_1 = e_trade_llm_gym_1.action_space\n",
        "\n",
        "# Observation and action spaces for PPO-DeepSeek 0.1%\n",
        "observation_space_llm_01 = e_trade_llm_gym_01.observation_space\n",
        "action_space_llm_01 = e_trade_llm_gym_01.action_space\n",
        "\n",
        "# Observation and action spaces for CPPO-DeepSeek 10% risk\n",
        "observation_space_llm_risk = e_trade_llm_risk_gym.observation_space\n",
        "action_space_llm_risk = e_trade_llm_risk_gym.action_space\n",
        "\n",
        "# Observation and action spaces for CPPO-DeepSeek 1% risk\n",
        "observation_space_llm_risk_1 = e_trade_llm_risk_gym_1.observation_space\n",
        "action_space_llm_risk_1 = e_trade_llm_risk_gym_1.action_space\n",
        "\n",
        "# Observation and action spaces for CPPO-DeepSeek 0.1% risk\n",
        "observation_space_llm_risk_01 = e_trade_llm_risk_gym_01.observation_space\n",
        "action_space_llm_risk_01 = e_trade_llm_risk_gym_01.action_space\n",
        "\n",
        "print(\"State shape:\", observation_space_llm.shape)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "from gymnasium.spaces import Box, Discrete\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "\n",
        "def combined_shape(length, shape=None):\n",
        "    if shape is None:\n",
        "        return (length,)\n",
        "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "\n",
        "def mlp(sizes, activation, output_activation=nn.Identity):\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def count_vars(module):\n",
        "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
        "\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "    \"\"\"\n",
        "    magic from rllab for computing discounted cumulative sums of vectors.\n",
        "\n",
        "    input:\n",
        "        vector x,\n",
        "        [x0,\n",
        "         x1,\n",
        "         x2]\n",
        "\n",
        "    output:\n",
        "        [x0 + discount * x1 + discount^2 * x2,\n",
        "         x1 + discount * x2,\n",
        "         x2]\n",
        "    \"\"\"\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def _distribution(self, obs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _log_prob_from_distribution(self, pi, act):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, obs, act=None):\n",
        "        # Produce action distributions for given observations, and\n",
        "        # optionally compute the log likelihood of given actions under\n",
        "        # those distributions.\n",
        "        pi = self._distribution(obs)\n",
        "        logp_a = None\n",
        "        if act is not None:\n",
        "            logp_a = self._log_prob_from_distribution(pi, act)\n",
        "        return pi, logp_a\n",
        "\n",
        "\n",
        "class MLPCategoricalActor(Actor):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
        "\n",
        "    def _distribution(self, obs):\n",
        "        logits = self.logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    def _log_prob_from_distribution(self, pi, act):\n",
        "        return pi.log_prob(act)\n",
        "\n",
        "\n",
        "class MLPGaussianActor(Actor):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
        "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
        "        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
        "\n",
        "    def _distribution(self, obs):\n",
        "        mu = self.mu_net(obs)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return Normal(mu, std)\n",
        "\n",
        "    def _log_prob_from_distribution(self, pi, act):\n",
        "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
        "\n",
        "\n",
        "class MLPCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
        "\n",
        "\n",
        "\n",
        "class MLPActorCritic(nn.Module):\n",
        "    def __init__(self, observation_space, action_space,\n",
        "                 hidden_sizes=(64, 64), activation=nn.Tanh):\n",
        "        super().__init__()\n",
        "\n",
        "        obs_dim = observation_space.shape[0]\n",
        "\n",
        "        # policy builder depends on action space\n",
        "        if isinstance(action_space, Box):\n",
        "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation)\n",
        "        elif isinstance(action_space, Discrete):\n",
        "            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation)\n",
        "\n",
        "        # build value function\n",
        "        self.v = MLPCritic(obs_dim, hidden_sizes, activation)\n",
        "\n",
        "    def step(self, obs):\n",
        "        with torch.no_grad():\n",
        "            pi = self.pi._distribution(obs)\n",
        "            a = pi.sample()\n",
        "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
        "            v = self.v(obs)\n",
        "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
        "\n",
        "    def act(self, obs):\n",
        "        return self.step(obs)[0]\n",
        "\n",
        "!dir"
      ],
      "metadata": {
        "id": "GL5fnsJFSBzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_ppo = MLPActorCritic(observation_space,action_space, hidden_sizes=(512, 512))\n",
        "print (loaded_ppo)\n",
        "loaded_ppo.load_state_dict(torch.load('/content/agent_ppo_100_epochs_20k_steps.pth'))\n",
        "#loaded_ppo.load_state_dict(torch.load('//content/agent_ppo_100_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_ppo.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Load the model\n",
        "loaded_cppo = MLPActorCritic(observation_space,action_space, hidden_sizes=(512, 512))\n",
        "loaded_cppo.load_state_dict(torch.load('/content/agent_cppo_100_epochs_20k_steps.pth'))\n",
        "#loaded_ppo.load_state_dict(torch.load('/kaggle/input/agent_cppo_25_epochs_20k_steps/pytorch/default/1/agent_ppo_25_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_cppo.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Load the model\n",
        "loaded_ppo_llm = MLPActorCritic(observation_space_llm,action_space_llm, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_ppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "#loaded_ppo_llm.load_state_dict(torch.load('/kaggle/input/agent_cppo_25_epochs_20k_steps/pytorch/default/1/agent_ppo_25_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_ppo_llm.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Load the model\n",
        "loaded_ppo_llama = MLPActorCritic(observation_space_llm,action_space_llm, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llama.load_state_dict(torch.load('/content/agent_ppo_llama_100_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_ppo_llm.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Load the model\n",
        "loaded_cppo_llm_risk = MLPActorCritic(observation_space_llm_risk,action_space_llm_risk, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk.load_state_dict(torch.load('/content/agent_cppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_cppo_llm_risk.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Load the PPO-DeepSeek 10% model\n",
        "loaded_ppo_llm = MLPActorCritic(observation_space_llm, action_space_llm, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm.load_state_dict(torch.load('/content/agent_ppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "loaded_ppo_llm.eval()  # Set the model to evaluation mode\n",
        "\n",
        "\n",
        "# Load the PPO-DeepSeek 1% model\n",
        "loaded_ppo_llm_1 = MLPActorCritic(observation_space_llm_1, action_space_llm_1, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm_1.load_state_dict(torch.load('/content/agent_ppo_deepseek_100_epochs_20k_steps_1.pth'))\n",
        "loaded_ppo_llm_1.eval()\n",
        "\n",
        "# Load the PPO-DeepSeek 0.1% model\n",
        "loaded_ppo_llm_01 = MLPActorCritic(observation_space_llm_01, action_space_llm_01, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm_01.load_state_dict(torch.load('/content/agent_ppo_deepseek_100_epochs_20k_steps_01.pth'))\n",
        "loaded_ppo_llm_01.eval()\n",
        "\n",
        "# Load the CPPO-DeepSeek 10% risk model\n",
        "loaded_cppo_llm_risk = MLPActorCritic(observation_space_llm_risk, action_space_llm_risk, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk.load_state_dict(torch.load('/content/agent_cppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "loaded_cppo_llm_risk.eval()\n",
        "\n",
        "# Load the CPPO-DeepSeek 1% risk model\n",
        "loaded_cppo_llm_risk_1 = MLPActorCritic(observation_space_llm_risk_1, action_space_llm_risk_1, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk_1.load_state_dict(torch.load('/content/agent_cppo_deepseek_100_epochs_20k_steps_1.pth'))\n",
        "loaded_cppo_llm_risk_1.eval()\n",
        "\n",
        "# Load the CPPO-DeepSeek 0.1% risk model\n",
        "loaded_cppo_llm_risk_01 = MLPActorCritic(observation_space_llm_risk_01, action_space_llm_risk_01, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk_01.load_state_dict(torch.load('/content/agent_cppo_deepseek_100_epochs_20k_steps_01.pth'))\n",
        "loaded_cppo_llm_risk_01.eval()\n",
        "\n",
        "# Load the model\n",
        "loaded_cppo_llama_risk = MLPActorCritic(observation_space_llm_risk,action_space_llm_risk, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llama_risk.load_state_dict(torch.load('/content/agent_deepseek_20_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_cppo_llama_risk.eval()  # Set the model to evaluation mode"
      ],
      "metadata": {
        "id": "i1QFY3t1T1Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def DRL_prediction(act, environment):\n",
        "    import torch\n",
        "    _torch = torch\n",
        "\n",
        "    state, _ = environment.reset()\n",
        "    account_memory = []  # To store portfolio values\n",
        "    actions_memory = []  # To store actions taken\n",
        "    portfolio_distribution = []  # To store portfolio distribution\n",
        "    episode_total_assets = [environment.initial_amount]\n",
        "\n",
        "    with _torch.no_grad():\n",
        "        for i in range(len(environment.df.index.unique())):\n",
        "            s_tensor = _torch.as_tensor((state,), dtype=torch.float32, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            a_tensor, _, _ = act.step(s_tensor)  # Compute action\n",
        "            action = a_tensor[0]  # Extract action\n",
        "\n",
        "            # Step through the environment\n",
        "            state, reward, done, _, _ = environment.step(action)\n",
        "\n",
        "            # Get stock prices for the current day\n",
        "            price_array = environment.df.loc[environment.day, \"close\"].values\n",
        "\n",
        "            # Stock holdings and cash balance\n",
        "            stock_holdings = environment.num_stock_shares\n",
        "            cash_balance = environment.asset_memory[-1]\n",
        "\n",
        "            # Calculate total portfolio value\n",
        "            total_asset = cash_balance + (price_array * stock_holdings).sum()\n",
        "\n",
        "            # Calculate portfolio distribution\n",
        "            stock_values = price_array * stock_holdings\n",
        "            total_invested = stock_values.sum()\n",
        "            distribution = stock_values / total_asset  # Fraction of each stock in the total portfolio\n",
        "            cash_fraction = cash_balance / total_asset\n",
        "\n",
        "            # Store results\n",
        "            episode_total_assets.append(total_asset)\n",
        "            account_memory.append(total_asset)\n",
        "            actions_memory.append(action)\n",
        "            portfolio_distribution.append({\"cash\": cash_fraction, \"stocks\": distribution.tolist()})\n",
        "\n",
        "       #     print(\"Total Asset Value:\", total_asset)\n",
        "        #    print(\"Portfolio Distribution:\", {\"cash\": cash_fraction, \"stocks\": distribution.tolist()})\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    print(\"Test Finished!\")\n",
        "    return episode_total_assets, account_memory, actions_memory, portfolio_distribution"
      ],
      "metadata": {
        "id": "e48j6FM2UXrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_assets_ppo, df_account_value_ppo, df_actions_ppo, df_portfolio_distribution_ppo = DRL_prediction(act=loaded_ppo, environment=e_trade_gym)\n",
        "#episode_total_assets, account_memory, actions_memory, portfolio_distribution = DRL_prediction(act=loaded_ppo, environment=e_trade_gym)\n",
        "\n",
        "df_assets_cppo, df_account_value_cppo, df_actions_cppo, df_portfolio_distribution_cppo = DRL_prediction(act=loaded_cppo, environment=e_trade_gym)\n",
        "\n",
        "# Prediction for PPO-DeepSeek 10%\n",
        "df_assets_ppo_llm, df_account_value_ppo_llm, df_actions_ppo_llm, df_portfolio_distribution_ppo_llm = DRL_prediction(\n",
        "    act=loaded_ppo_llm, environment=e_trade_llm_gym\n",
        ")\n",
        "\n",
        "# Prediction for PPO-DeepSeek 1%\n",
        "df_assets_ppo_llm_1, df_account_value_ppo_llm_1, df_actions_ppo_llm_1, df_portfolio_distribution_ppo_llm_1 = DRL_prediction(\n",
        "    act=loaded_ppo_llm_1, environment=e_trade_llm_gym_1\n",
        ")\n",
        "\n",
        "# Prediction for PPO-DeepSeek 0.1%\n",
        "df_assets_ppo_llm_01, df_account_value_ppo_llm_01, df_actions_ppo_llm_01, df_portfolio_distribution_ppo_llm_01 = DRL_prediction(\n",
        "    act=loaded_ppo_llm_01, environment=e_trade_llm_gym_01\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 10% risk\n",
        "df_assets_cppo_llm_risk, df_account_value_cppo_llm_risk, df_actions_cppo_llm_risk, df_portfolio_distribution_cppo_llm_risk = DRL_prediction(\n",
        "    act=loaded_cppo_llm_risk, environment=e_trade_llm_risk_gym\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 1% risk\n",
        "df_assets_cppo_llm_risk_1, df_account_value_cppo_llm_risk_1, df_actions_cppo_llm_risk_1, df_portfolio_distribution_cppo_llm_risk_1 = DRL_prediction(\n",
        "   act=loaded_cppo_llm_risk_1, environment=e_trade_llm_risk_gym_1\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 0.1% risk\n",
        "df_assets_cppo_llm_risk_01, df_account_value_cppo_llm_risk_01, df_actions_cppo_llm_risk_01, df_portfolio_distribution_cppo_llm_risk_01 = DRL_prediction(\n",
        "   act=loaded_cppo_llm_risk_01, environment=e_trade_llm_risk_gym_01\n",
        ")\n",
        "\n",
        "# Prediction for PPO-DeepSeek 10%\n",
        "df_assets_ppo_llm, df_account_value_ppo_llm, df_actions_ppo_llm, df_portfolio_distribution_ppo_llm = DRL_prediction(\n",
        "    act=loaded_ppo_llm, environment=e_trade_llm_gym\n",
        ")\n",
        "\n",
        "# Prediction for PPO-DeepSeek 1%\n",
        "df_assets_ppo_llm_1, df_account_value_ppo_llm_1, df_actions_ppo_llm_1, df_portfolio_distribution_ppo_llm_1 = DRL_prediction(\n",
        "    act=loaded_ppo_llm_1, environment=e_trade_llm_gym_1\n",
        ")\n",
        "\n",
        "# Prediction for PPO-DeepSeek 0.1%\n",
        "df_assets_ppo_llm_01, df_account_value_ppo_llm_01, df_actions_ppo_llm_01, df_portfolio_distribution_ppo_llm_01 = DRL_prediction(\n",
        "    act=loaded_ppo_llm_01, environment=e_trade_llm_gym_01\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 10% risk\n",
        "df_assets_cppo_llm_risk, df_account_value_cppo_llm_risk, df_actions_cppo_llm_risk, df_portfolio_distribution_cppo_llm_risk = DRL_prediction(\n",
        "    act=loaded_cppo_llm_risk, environment=e_trade_llm_risk_gym\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 1% risk\n",
        "df_assets_cppo_llm_risk_1, df_account_value_cppo_llm_risk_1, df_actions_cppo_llm_risk_1, df_portfolio_distribution_cppo_llm_risk_1 = DRL_prediction(\n",
        "   act=loaded_cppo_llm_risk_1, environment=e_trade_llm_risk_gym_1\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 0.1% risk\n",
        "df_assets_cppo_llm_risk_01, df_account_value_cppo_llm_risk_01, df_actions_cppo_llm_risk_01, df_portfolio_distribution_cppo_llm_risk_01 = DRL_prediction(\n",
        "   act=loaded_cppo_llm_risk_01, environment=e_trade_llm_risk_gym_01\n",
        ")\n",
        "\n",
        "df_assets_ppo_llm, df_account_value_ppo_llm, df_actions_ppo_llm, df_portfolio_distribution_ppo_llm = DRL_prediction(act=loaded_ppo_llm, environment=e_trade_llm_gym)\n",
        "\n",
        "df_assets_ppo_llama, df_account_value_ppo_llama, df_actions_ppo_llama, df_portfolio_distribution_ppo_llama= DRL_prediction(act=loaded_ppo_llama, environment=e_trade_llm_gym)\n",
        "\n",
        "df_assets_cppo_llm_risk, df_account_value_cppo_llm_risk, df_actions_cppo_llm_risk, df_portfolio_distribution_cppo_llm_risk = DRL_prediction(act=loaded_cppo_llm_risk, environment=e_trade_llm_risk_gym)\n",
        "\n",
        "df_assets_cppo_llama_risk, df_account_value_cppo_llama_risk, df_actions_cppo_llama_risk, df_portfolio_distribution_cppo_llama_risk = DRL_prediction(act=loaded_cppo_llama_risk, environment=e_trade_llm_risk_gym)\n"
      ],
      "metadata": {
        "id": "hazlzyoTVVhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CXRgQPjj7NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few samples from each list\n",
        "print(\"\\nðŸ“ˆ df_assets_ppo_llm (sample):\", df_assets_ppo_llm[:5])\n",
        "print(\"\\nðŸ’° df_account_value_ppo_llm (sample):\", df_account_value_ppo_llm[:5])\n",
        "print(\"\\nðŸ§  df_actions_ppo_llm (sample):\", df_actions_ppo_llm[:2])  # usually multidimensional\n",
        "print(\"\\nðŸ“Š df_portfolio_distribution_ppo_llm (sample):\", df_portfolio_distribution_ppo[:2])\n"
      ],
      "metadata": {
        "id": "4TvTCCkmfjk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_assets = pd.Series(df_assets_cppo_llama_risk, name=\"Asset Value\")\n",
        "df_assets.plot(title=\"Sample Total Asset Value Over Time\", figsize=(10, 4))\n",
        "plt.ylabel(\"Value ($)\")\n",
        "plt.xlabel(\"Timestep\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p80js3olgeuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "actions_array = np.array(df_actions_ppo_llm)\n",
        "df_actions = pd.DataFrame(actions_array)\n",
        "\n",
        "# Plot action distribution for a few stocks\n",
        "df_actions.iloc[:, :5].plot(figsize=(12, 5), title=\"Actions for First 5 Stocks\")\n",
        "plt.xlabel(\"Timestep\")\n",
        "plt.ylabel(\"Action Value\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZfGyugaFgl6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Trader in a Day.ipynb",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "isSourceIdPinned": true,
          "modelId": 222180,
          "modelInstanceId": 200368,
          "sourceId": 234579,
          "sourceType": "modelInstanceVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 222227,
          "modelInstanceId": 200415,
          "sourceId": 234637,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30839,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}